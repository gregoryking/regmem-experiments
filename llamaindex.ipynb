{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-llms-ollama in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.1.3)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-llms-ollama) (0.10.36)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.5.9)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.26.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.28.0)\n",
      "Requirement already satisfied: pandas in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.9.4)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.7.1)\n",
      "Requirement already satisfied: anyio in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2024.5.10)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2023.4)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting llama-index-program-lmformatenforcer\n",
      "  Downloading llama_index_program_lmformatenforcer-0.1.2-py3-none-any.whl.metadata (795 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-program-lmformatenforcer) (0.10.36)\n",
      "Collecting llama-index-llms-huggingface<0.2.0,>=0.1.1 (from llama-index-program-lmformatenforcer)\n",
      "  Downloading llama_index_llms_huggingface-0.1.5-py3-none-any.whl.metadata (789 bytes)\n",
      "Collecting llama-index-llms-llama-cpp<0.2.0,>=0.1.1 (from llama-index-program-lmformatenforcer)\n",
      "  Downloading llama_index_llms_llama_cpp-0.1.3-py3-none-any.whl.metadata (695 bytes)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.5.9)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.26.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.28.0)\n",
      "Requirement already satisfied: pandas in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.16.0)\n",
      "Collecting huggingface-hub<0.21.0,>=0.20.3 (from llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer)\n",
      "  Downloading text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (2.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (4.40.2)\n",
      "Collecting llama-cpp-python<0.3.0,>=0.2.32 (from llama-index-llms-llama-cpp<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer)\n",
      "  Downloading llama_cpp_python-0.2.73.tar.gz (49.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.9.4)\n",
      "Requirement already satisfied: filelock in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (23.2)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (3.1.3)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2.7.1)\n",
      "Requirement already satisfied: anyio in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2024.5.10)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (3.0.3)\n",
      "Requirement already satisfied: sympy in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (1.12)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (0.4.3)\n",
      "Collecting accelerate>=0.21.0 (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer)\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2023.4)\n",
      "Requirement already satisfied: psutil in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (5.9.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from jinja2>=2.11.3->llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (2.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-program-lmformatenforcer) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.2.0,>=0.1.1->llama-index-program-lmformatenforcer) (1.3.0)\n",
      "Downloading llama_index_program_lmformatenforcer-0.1.2-py3-none-any.whl (3.8 kB)\n",
      "Downloading llama_index_llms_huggingface-0.1.5-py3-none-any.whl (10 kB)\n",
      "Downloading llama_index_llms_llama_cpp-0.1.3-py3-none-any.whl (5.1 kB)\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.73-cp312-cp312-macosx_14_0_arm64.whl size=3469930 sha256=9210c3aa63e64b5e0f0ce3680d440fe682022398fb145b0e649acbde227e1443\n",
      "  Stored in directory: /Users/greg/Library/Caches/pip/wheels/44/8f/83/bed7814c35c6945f720c80bd458bf0f4a053cd551221e4e6a6\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python, huggingface-hub, text-generation, accelerate, llama-index-llms-llama-cpp, llama-index-llms-huggingface, llama-index-program-lmformatenforcer\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.0\n",
      "    Uninstalling huggingface-hub-0.23.0:\n",
      "      Successfully uninstalled huggingface-hub-0.23.0\n",
      "Successfully installed accelerate-0.30.1 diskcache-5.6.3 huggingface-hub-0.20.3 llama-cpp-python-0.2.73 llama-index-llms-huggingface-0.1.5 llama-index-llms-llama-cpp-0.1.3 llama-index-program-lmformatenforcer-0.1.2 text-generation-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-llms-llama-cpp in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.1.3)\n",
      "Requirement already satisfied: llama-cpp-python<0.3.0,>=0.2.32 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-llms-llama-cpp) (0.2.73)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-llms-llama-cpp) (0.10.36)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (4.10.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (1.26.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (3.1.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.5.9)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.8.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.28.0)\n",
      "Requirement already satisfied: pandas in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (4.66.4)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from jinja2>=2.11.3->llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (2.1.5)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.7.1)\n",
      "Requirement already satisfied: anyio in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.5.10)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2023.4)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-ollama\n",
    "%pip install llama-index-program-lmformatenforcer\n",
    "%pip install llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.10.36-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: lm-format-enforcer in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.10.1)\n",
      "Requirement already satisfied: llama-cpp-python in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.2.73)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.2.4-py3-none-any.whl.metadata (678 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.35 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index) (0.10.36)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl.metadata (603 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.1.18-py3-none-any.whl.metadata (559 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.1.22-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from lm-format-enforcer) (0.3.3)\n",
      "Requirement already satisfied: packaging in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from lm-format-enforcer) (23.2)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from lm-format-enforcer) (2.7.1)\n",
      "Requirement already satisfied: pyyaml in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from lm-format-enforcer) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-cpp-python) (4.10.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-cpp-python) (1.26.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Requirement already satisfied: openai>=1.14.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.28.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.5.9)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.8.1)\n",
      "Requirement already satisfied: pandas in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (4.66.4)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading llama_parse-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic>=1.10.8->lm-format-enforcer) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic>=1.10.8->lm-format-enforcer) (2.18.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.5.10)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.16.0)\n",
      "Downloading llama_index-0.10.36-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_agent_openai-0.2.4-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl (6.0 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.1.18-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl (5.8 kB)\n",
      "Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
      "Downloading llama_parse-0.4.2-py3-none-any.whl (7.6 kB)\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: striprtf, pypdf, llama-index-legacy, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed llama-index-0.10.36 llama-index-agent-openai-0.2.4 llama-index-cli-0.1.12 llama-index-embeddings-openai-0.1.9 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.18 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.22 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.2 pypdf-4.2.0 striprtf-0.0.26\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index lm-format-enforcer llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "from llama_index.program.lmformatenforcer import (\n",
    "    LMFormatEnforcerPydanticProgram,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/xcsrw32d7mnbpc723_dd8_f00000gp/T/ipykernel_53456/3725947808.py:57: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  @validator('registered_date', pre=True, always=True)\n",
      "/var/folders/1m/xcsrw32d7mnbpc723_dd8_f00000gp/T/ipykernel_53456/3725947808.py:66: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  @validator('donation', 'interest', pre=True)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, validator\n",
    "from typing import Union\n",
    "from datetime import date, datetime\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "class DateEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, date):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)\n",
    "\n",
    "class Address(BaseModel):\n",
    "    street: Optional[str] = None\n",
    "    city: Optional[str] = None\n",
    "    state: Optional[str] = None\n",
    "    postal_code: Optional[str] = None\n",
    "    country: Optional[str] = None\n",
    "\n",
    "    def json(self, *args, **kwargs):\n",
    "        return json.dumps(self.dict(), cls=DateEncoder)\n",
    "\n",
    "class Donor(BaseModel):\n",
    "    name: Optional[str] = None\n",
    "    address: Optional[Address] = None\n",
    "\n",
    "    def json(self, *args, **kwargs):\n",
    "        return json.dumps(self.dict(), cls=DateEncoder)\n",
    "\n",
    "class Money(BaseModel):\n",
    "    value: Optional[float] = None\n",
    "    currency: Optional[str] = None\n",
    "\n",
    "    def json(self, *args, **kwargs):\n",
    "        return json.dumps(self.dict(), cls=DateEncoder)\n",
    "\n",
    "class Donation(BaseModel):\n",
    "    donor: Optional[Donor] = None\n",
    "    amount: Optional[Money] = None\n",
    "    date: Optional[datetime] = None\n",
    "    description: Optional[str] = None\n",
    "\n",
    "    def json(self, *args, **kwargs):\n",
    "        return json.dumps(self.dict(), cls=DateEncoder)\n",
    "\n",
    "class Interest(BaseModel):\n",
    "    description: Optional[str] = None\n",
    "\n",
    "    def json(self, *args, **kwargs):\n",
    "        return json.dumps(self.dict(), cls=DateEncoder)\n",
    "\n",
    "class Item(BaseModel):\n",
    "    donation: Optional[Union[Donation, None]] = None\n",
    "    interest: Optional[Union[Interest, None]] = None\n",
    "    registered_date: Optional[date] = None\n",
    "\n",
    "    @validator('registered_date', pre=True, always=True)\n",
    "    def parse_date(cls, value):\n",
    "        if value is None:\n",
    "            return None\n",
    "        try:\n",
    "            return datetime.strptime(value, '%Y-%m-%d').date()\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    @validator('donation', 'interest', pre=True)\n",
    "    def check_exclusive(cls, v, values):\n",
    "        if v is not None and values.get('interest') is not None:\n",
    "            raise ValueError(\"An item can only be either a donation or an interest, not both.\")\n",
    "        return v\n",
    "\n",
    "    def json(self, *args, **kwargs):\n",
    "        return json.dumps(self.dict(), cls=DateEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/greg/Library/Caches/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.18 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3200.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   368.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "llm = LlamaCPP(temperature=0.0)\n",
    "\n",
    "ITEM = \"\"\"\"Name of donor: Harlequin Football Club Ltd\n",
    "Address of donor: Twickenham Stoop Stadium, Langhorn Drive, Twickenham TW2 7SX\n",
    "Amount of donation or nature and value if donation in kind: Two tickets with hospitality for the BG14 match at Twickenham Stadium, value Â£900\n",
    "Date received: 4 March 2023\n",
    "Date accepted: 4 March 2023\n",
    "Donor status: company, registration 03213073\n",
    "(Registered 06 March 2023)\"\"\"\n",
    "\n",
    "program = LMFormatEnforcerPydanticProgram(\n",
    "    output_cls=Item,\n",
    "    prompt_template_str=(\n",
    "        \"Your response should be according to the following json schema: \\n\"\n",
    "        \"{json_schema}\\n\"\n",
    "        \"\"\"You are AI assistant that extracts entities from the free text descriptions of MP's items.\n",
    "\n",
    "                   The items are either donations or interests. If there is no financial value, then assume it is an interest.\n",
    "\n",
    "                   Ensure to record a description of the donation or interest.\n",
    "\n",
    "                   Ensure you extract a financial value of donation where available.\n",
    "                   \n",
    "                   If any parts of the template don't look relevant, leave them blank.\n",
    "                \n",
    "                   Try and record the dates items are registered, usually you can find this by looking for the word \"Registered\" followed by the date.\n",
    "\n",
    "                   Ensure dates are always in ISO format.\n",
    "                   \n",
    "                   Do NOT make up information if you are unsure.\n",
    "\n",
    "                   Output nothing other than valid JSON.\n",
    "\n",
    "                   \"\"\"\n",
    "        \"This is the item to analyse:\\n\\n\"\n",
    "        \"{item}\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unknown LMFormatEnforcer Problem. Prefix: 'Your response should be according to the following json schema: \n",
      "{\"$defs\": {\"Address\": {\"properties\": {\"street\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Street\"}, \"city\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"City\"}, \"state\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"State\"}, \"postal_code\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Postal Code\"}, \"country\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Country\"}}, \"title\": \"Address\", \"type\": \"object\"}, \"Donation\": {\"properties\": {\"donor\": {\"anyOf\": [{\"$ref\": \"#/$defs/Donor\"}, {\"type\": \"null\"}], \"default\": null}, \"amount\": {\"allOf\": [{\"$ref\": \"#/$defs/Money\"}], \"default\": null}, \"date\": {\"anyOf\": [{\"format\": \"date-time\", \"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Date\"}, \"description\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Description\"}}, \"title\": \"Donation\", \"type\": \"object\"}, \"Donor\": {\"properties\": {\"name\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Name\"}, \"address\": {\"anyOf\": [{\"$ref\": \"#/$defs/Address\"}, {\"type\": \"null\"}], \"default\": null}}, \"title\": \"Donor\", \"type\": \"object\"}, \"Interest\": {\"properties\": {\"description\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Description\"}}, \"title\": \"Interest\", \"type\": \"object\"}, \"Money\": {\"properties\": {\"value\": {\"anyOf\": [{\"type\": \"number\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Value\"}, \"currency\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Currency\"}}, \"title\": \"Money\", \"type\": \"object\"}}, \"properties\": {\"donation\": {\"anyOf\": [{\"$ref\": \"#/$defs/Donation\"}, {\"type\": \"null\"}], \"default\": null}, \"interest\": {\"anyOf\": [{\"$ref\": \"#/$defs/Interest\"}, {\"type\": \"null\"}], \"default\": null}, \"registered_date\": {\"anyOf\": [{\"format\": \"date\", \"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Registered Date\"}}, \"title\": \"Item\", \"type\": \"object\"}\n",
      "You are AI assistant that extracts entities from the free text descriptions of MP's items.\n",
      "\n",
      "                   The items are either donations or interests. If there is no financial value, then assume it is an interest.\n",
      "\n",
      "                   Ensure to record a description of the donation or interest.\n",
      "\n",
      "                   Ensure you extract a financial value of donation where available.\n",
      "                   \n",
      "                   If any parts of the template don't look relevant, leave them blank.\n",
      "                \n",
      "                   Try and record the dates items are registered, usually you can find this by looking for the word \"Registered\" followed by the date.\n",
      "\n",
      "                   Ensure dates are always in ISO format.\n",
      "                   \n",
      "                   Do NOT make up information if you are unsure.\n",
      "\n",
      "                   Output nothing other than valid JSON.\n",
      "\n",
      "                   This is the item to analyse:\n",
      "\n",
      "Name of donor: Ministry of Foreign Affairs of the State of Qatar\n",
      "Address of donor: Ministry of Foreign Affairs, Almirqab Tower, West Bay, Doha, Qatar\n",
      "Estimate of the probable value (or amount of any donation): Return flights, accommodation and food, estimated value Â£6,872\n",
      "Destination of visit: Doha, Qatar\n",
      "Dates of visit: 19-23 September 2023\n",
      "Purpose of visit: Qatar APPG visit to meet with ministers, the Shura Council and senior local and international stakeholders in international development to explore ways of enhancing UK-Qatar cooperation on global development and aid.\n",
      "(Registered 16 October 2023)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\"donation\": {\n",
      "\"amount'\n",
      "Terminating the parser. Please open an issue at \n",
      "https://github.com/noamgat/lm-format-enforcer/issues with the prefix and CharacterLevelParser parameters\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages/lmformatenforcer/tokenenforcer.py\", line 96, in _compute_allowed_tokens\n",
      "    self._collect_allowed_tokens(state.parser, self.tokenizer_tree.root, allowed_tokens, shortcut_key)\n",
      "  File \"/Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages/lmformatenforcer/tokenenforcer.py\", line 144, in _collect_allowed_tokens\n",
      "    self._collect_allowed_tokens(next_parser, next_tree_node, allowed_tokens, None)\n",
      "  File \"/Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages/lmformatenforcer/tokenenforcer.py\", line 142, in _collect_allowed_tokens\n",
      "    next_parser = parser.add_character(character)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages/lmformatenforcer/jsonschemaparser.py\", line 74, in add_character\n",
      "    updated_parser.object_stack[receiving_idx] = updated_parser.object_stack[receiving_idx].add_character(new_character)\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages/lmformatenforcer/jsonschemaparser.py\", line 334, in add_character\n",
      "    self.current_key_parser = get_parser(\n",
      "                              ^^^^^^^^^^^\n",
      "  File \"/Users/greg/.pyenv/versions/3.12.1/lib/python3.12/site-packages/lmformatenforcer/jsonschemaparser.py\", line 252, in get_parser\n",
      "    raise Exception(\"Unsupported type \" + str(value_schema.type))\n",
      "Exception: Unsupported type None\n",
      "\n",
      "llama_print_timings:        load time =    7482.81 ms\n",
      "llama_print_timings:      sample time =       2.12 ms /    23 runs   (    0.09 ms per token, 10854.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17294.67 ms /  1069 tokens (   16.18 ms per token,    61.81 tokens per second)\n",
      "llama_print_timings:        eval time =    1880.86 ms /    22 runs   (   85.49 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =   19219.41 ms /  1091 tokens\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Item\n__root__\n  Unterminated string starting at: line 15 column 1 (char 28) [type=value_error.jsondecode, input_value='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...n\"donation\": {\\n\"amount', input_type=str]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/pydantic/main.py:1143\u001b[0m, in \u001b[0;36mBaseModel.parse_raw\u001b[0;34m(cls, b, content_type, encoding, proto, allow_pickle)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1143\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_str_bytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/pydantic/deprecated/parse.py:49\u001b[0m, in \u001b[0;36mload_str_bytes\u001b[0;34m(b, content_type, encoding, proto, allow_pickle, json_loads)\u001b[0m\n\u001b[1;32m     48\u001b[0m         b \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mdecode(encoding)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m proto \u001b[38;5;241m==\u001b[39m Protocol\u001b[38;5;241m.\u001b[39mpickle:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 15 column 1 (char 28)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[350], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m expense_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName of donor: Ministry of Foreign Affairs of the State of Qatar\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAddress of donor: Ministry of Foreign Affairs, Almirqab Tower, West Bay, Doha, Qatar\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEstimate of the probable value (or amount of any donation): Return flights, accommodation and food, estimated value \u001b[39m\u001b[38;5;130;01m\\u00c2\u001b[39;00m\u001b[38;5;130;01m\\u00a3\u001b[39;00m\u001b[38;5;124m6,872\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDestination of visit: Doha, Qatar\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDates of visit: 19-23 September 2023\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPurpose of visit: Qatar APPG visit to meet with ministers, the Shura Council and senior local and international stakeholders in international development to explore ways of enhancing UK-Qatar cooperation on global development and aid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m(Registered 16 October 2023)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpense_description\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/llama_index/program/lmformatenforcer/base.py:103\u001b[0m, in \u001b[0;36mLMFormatEnforcerPydanticProgram.__call__\u001b[0;34m(self, llm_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcomplete(full_str, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_kwargs)\n\u001b[1;32m    102\u001b[0m text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/pydantic/main.py:1170\u001b[0m, in \u001b[0;36mBaseModel.parse_raw\u001b[0;34m(cls, b, content_type, encoding, proto, allow_pickle)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;66;03m# ctx is missing here, but since we've added `input` to the error, we're not pretending it's the same\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     error: pydantic_core\u001b[38;5;241m.\u001b[39mInitErrorDetails \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;66;03m# The type: ignore on the next line is to ignore the requirement of LiteralString\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: pydantic_core\u001b[38;5;241m.\u001b[39mPydanticCustomError(type_str, \u001b[38;5;28mstr\u001b[39m(exc)),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m'\u001b[39m,),\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: b,\n\u001b[1;32m   1169\u001b[0m     }\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m pydantic_core\u001b[38;5;241m.\u001b[39mValidationError\u001b[38;5;241m.\u001b[39mfrom_exception_data(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, [error])\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_validate(obj)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Item\n__root__\n  Unterminated string starting at: line 15 column 1 (char 28) [type=value_error.jsondecode, input_value='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...n\"donation\": {\\n\"amount', input_type=str]"
     ]
    }
   ],
   "source": [
    "expense_description = \"Name of donor: Ministry of Foreign Affairs of the State of Qatar\\nAddress of donor: Ministry of Foreign Affairs, Almirqab Tower, West Bay, Doha, Qatar\\nEstimate of the probable value (or amount of any donation): Return flights, accommodation and food, estimated value \\u00c2\\u00a36,872\\nDestination of visit: Doha, Qatar\\nDates of visit: 19-23 September 2023\\nPurpose of visit: Qatar APPG visit to meet with ministers, the Shura Council and senior local and international stakeholders in international development to explore ways of enhancing UK-Qatar cooperation on global development and aid.\\n(Registered 16 October 2023)\"\n",
    "\n",
    "output = program(item=expense_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(donation=Donation(donor=Donor(name='Ministry of Foreign Affairs of the State of Qatar', address=Address(street='Ministry of Foreign Affairs, Almirqab Tower, West Bay', city='Doha', state='Qatar', postal_code='Qatar', country='Qatar')), amount=None, date=None, description='Return flights, accommodation and food, estimated value £6,872'), interest=None, registered_date=datetime.date(2023, 10, 16))"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"donation\": null, \"interest\": {\"description\": \"Member of the Teesside Freeport Board\"}, \"registered_date\": null}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "test = output.json(indent=None, separators=(',', ':'))\n",
    "print(test)\n",
    "with open(file_path, 'a') as file:\n",
    "    file.write(f\"{test}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       8.68 ms /    85 runs   (    0.10 ms per token,  9788.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =  470178.72 ms /   168 tokens ( 2798.68 ms per token,     0.36 tokens per second)\n",
      "llama_print_timings:        eval time =    7809.98 ms /    84 runs   (   92.98 ms per token,    10.76 tokens per second)\n",
      "llama_print_timings:       total time =    8052.84 ms /   252 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       7.63 ms /    77 runs   (    0.10 ms per token, 10093.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2129.24 ms /    42 tokens (   50.70 ms per token,    19.73 tokens per second)\n",
      "llama_print_timings:        eval time =    6508.50 ms /    76 runs   (   85.64 ms per token,    11.68 tokens per second)\n",
      "llama_print_timings:       total time =    8773.54 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       8.12 ms /    80 runs   (    0.10 ms per token,  9851.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2094.18 ms /    34 tokens (   61.59 ms per token,    16.24 tokens per second)\n",
      "llama_print_timings:        eval time =    6755.71 ms /    79 runs   (   85.52 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:       total time =    8994.24 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       9.02 ms /   105 runs   (    0.09 ms per token, 11638.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2101.21 ms /    38 tokens (   55.30 ms per token,    18.08 tokens per second)\n",
      "llama_print_timings:        eval time =    8600.43 ms /   104 runs   (   82.70 ms per token,    12.09 tokens per second)\n",
      "llama_print_timings:       total time =   10864.01 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       9.45 ms /   105 runs   (    0.09 ms per token, 11109.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     826.43 ms /    18 tokens (   45.91 ms per token,    21.78 tokens per second)\n",
      "llama_print_timings:        eval time =    8832.55 ms /   104 runs   (   84.93 ms per token,    11.77 tokens per second)\n",
      "llama_print_timings:       total time =    9824.41 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       9.30 ms /   105 runs   (    0.09 ms per token, 11286.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2157.46 ms /    38 tokens (   56.78 ms per token,    17.61 tokens per second)\n",
      "llama_print_timings:        eval time =    8665.72 ms /   104 runs   (   83.32 ms per token,    12.00 tokens per second)\n",
      "llama_print_timings:       total time =   10986.80 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      18.46 ms /   226 runs   (    0.08 ms per token, 12239.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1358.39 ms /    27 tokens (   50.31 ms per token,    19.88 tokens per second)\n",
      "llama_print_timings:        eval time =   19240.07 ms /   225 runs   (   85.51 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:       total time =   20959.36 ms /   252 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       8.02 ms /    83 runs   (    0.10 ms per token, 10345.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2121.67 ms /    40 tokens (   53.04 ms per token,    18.85 tokens per second)\n",
      "llama_print_timings:        eval time =    6823.39 ms /    82 runs   (   83.21 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =    9091.18 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       8.44 ms /    91 runs   (    0.09 ms per token, 10780.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1888.47 ms /    32 tokens (   59.01 ms per token,    16.94 tokens per second)\n",
      "llama_print_timings:        eval time =    7407.57 ms /    90 runs   (   82.31 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =    9453.20 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       8.97 ms /   104 runs   (    0.09 ms per token, 11591.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2088.44 ms /    33 tokens (   63.29 ms per token,    15.80 tokens per second)\n",
      "llama_print_timings:        eval time =    8556.54 ms /   103 runs   (   83.07 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =   10824.54 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       9.49 ms /    98 runs   (    0.10 ms per token, 10322.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2123.61 ms /    39 tokens (   54.45 ms per token,    18.36 tokens per second)\n",
      "llama_print_timings:        eval time =    7968.90 ms /    97 runs   (   82.15 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:       total time =   10271.00 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      11.12 ms /   123 runs   (    0.09 ms per token, 11060.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2218.66 ms /    64 tokens (   34.67 ms per token,    28.85 tokens per second)\n",
      "llama_print_timings:        eval time =   10672.36 ms /   122 runs   (   87.48 ms per token,    11.43 tokens per second)\n",
      "llama_print_timings:       total time =   13124.13 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      15.34 ms /   164 runs   (    0.09 ms per token, 10689.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3209.58 ms /   141 tokens (   22.76 ms per token,    43.93 tokens per second)\n",
      "llama_print_timings:        eval time =   13795.19 ms /   163 runs   (   84.63 ms per token,    11.82 tokens per second)\n",
      "llama_print_timings:       total time =   17314.57 ms /   304 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       8.93 ms /   104 runs   (    0.09 ms per token, 11650.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2886.10 ms /   118 tokens (   24.46 ms per token,    40.89 tokens per second)\n",
      "llama_print_timings:        eval time =    8794.57 ms /   103 runs   (   85.38 ms per token,    11.71 tokens per second)\n",
      "llama_print_timings:       total time =   11849.43 ms /   221 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       8.69 ms /   105 runs   (    0.08 ms per token, 12080.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2551.90 ms /    91 tokens (   28.04 ms per token,    35.66 tokens per second)\n",
      "llama_print_timings:        eval time =    8703.69 ms /   104 runs   (   83.69 ms per token,    11.95 tokens per second)\n",
      "llama_print_timings:       total time =   11418.35 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       6.63 ms /    81 runs   (    0.08 ms per token, 12224.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2859.88 ms /   118 tokens (   24.24 ms per token,    41.26 tokens per second)\n",
      "llama_print_timings:        eval time =    6851.74 ms /    80 runs   (   85.65 ms per token,    11.68 tokens per second)\n",
      "llama_print_timings:       total time =    9837.26 ms /   198 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       6.96 ms /    83 runs   (    0.08 ms per token, 11920.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2613.38 ms /    93 tokens (   28.10 ms per token,    35.59 tokens per second)\n",
      "llama_print_timings:        eval time =    6825.87 ms /    82 runs   (   83.24 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:       total time =    9568.62 ms /   175 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       9.51 ms /   106 runs   (    0.09 ms per token, 11152.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2863.24 ms /   112 tokens (   25.56 ms per token,    39.12 tokens per second)\n",
      "llama_print_timings:        eval time =    8814.85 ms /   105 runs   (   83.95 ms per token,    11.91 tokens per second)\n",
      "llama_print_timings:       total time =   11842.46 ms /   217 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       8.22 ms /    91 runs   (    0.09 ms per token, 11075.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2471.06 ms /    73 tokens (   33.85 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:        eval time =    7562.98 ms /    90 runs   (   84.03 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =   10198.47 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      10.24 ms /   114 runs   (    0.09 ms per token, 11136.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2441.93 ms /    65 tokens (   37.57 ms per token,    26.62 tokens per second)\n",
      "llama_print_timings:        eval time =    9517.42 ms /   113 runs   (   84.22 ms per token,    11.87 tokens per second)\n",
      "llama_print_timings:       total time =   12161.24 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       7.29 ms /    78 runs   (    0.09 ms per token, 10696.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2131.23 ms /    41 tokens (   51.98 ms per token,    19.24 tokens per second)\n",
      "llama_print_timings:        eval time =    6348.89 ms /    77 runs   (   82.45 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    8612.34 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       9.89 ms /   113 runs   (    0.09 ms per token, 11424.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2202.03 ms /    58 tokens (   37.97 ms per token,    26.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9684.70 ms /   112 runs   (   86.47 ms per token,    11.56 tokens per second)\n",
      "llama_print_timings:       total time =   12068.82 ms /   170 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       7.62 ms /    82 runs   (    0.09 ms per token, 10763.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2470.52 ms /    73 tokens (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:        eval time =    6815.92 ms /    81 runs   (   84.15 ms per token,    11.88 tokens per second)\n",
      "llama_print_timings:       total time =    9411.23 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       7.52 ms /    80 runs   (    0.09 ms per token, 10635.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2507.47 ms /    79 tokens (   31.74 ms per token,    31.51 tokens per second)\n",
      "llama_print_timings:        eval time =    6589.05 ms /    79 runs   (   83.41 ms per token,    11.99 tokens per second)\n",
      "llama_print_timings:       total time =    9230.21 ms /   158 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      10.15 ms /   117 runs   (    0.09 ms per token, 11530.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2192.97 ms /    54 tokens (   40.61 ms per token,    24.62 tokens per second)\n",
      "llama_print_timings:        eval time =    9707.04 ms /   116 runs   (   83.68 ms per token,    11.95 tokens per second)\n",
      "llama_print_timings:       total time =   12092.44 ms /   170 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       7.58 ms /    82 runs   (    0.09 ms per token, 10812.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2542.39 ms /    73 tokens (   34.83 ms per token,    28.71 tokens per second)\n",
      "llama_print_timings:        eval time =    6844.86 ms /    81 runs   (   84.50 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =    9513.57 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      16.83 ms /   180 runs   (    0.09 ms per token, 10696.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3658.69 ms /   167 tokens (   21.91 ms per token,    45.64 tokens per second)\n",
      "llama_print_timings:        eval time =   15580.05 ms /   179 runs   (   87.04 ms per token,    11.49 tokens per second)\n",
      "llama_print_timings:       total time =   19565.09 ms /   346 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      11.12 ms /   122 runs   (    0.09 ms per token, 10970.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3643.00 ms /   165 tokens (   22.08 ms per token,    45.29 tokens per second)\n",
      "llama_print_timings:        eval time =   10449.53 ms /   121 runs   (   86.36 ms per token,    11.58 tokens per second)\n",
      "llama_print_timings:       total time =   14302.54 ms /   286 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       6.28 ms /    63 runs   (    0.10 ms per token, 10036.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     599.69 ms /    10 tokens (   59.97 ms per token,    16.68 tokens per second)\n",
      "llama_print_timings:        eval time =    5095.22 ms /    62 runs   (   82.18 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:       total time =    5796.06 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       9.88 ms /   112 runs   (    0.09 ms per token, 11337.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2561.57 ms /    91 tokens (   28.15 ms per token,    35.53 tokens per second)\n",
      "llama_print_timings:        eval time =    9245.84 ms /   111 runs   (   83.30 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:       total time =   12005.51 ms /   202 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =       9.17 ms /    96 runs   (    0.10 ms per token, 10468.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2183.36 ms /    51 tokens (   42.81 ms per token,    23.36 tokens per second)\n",
      "llama_print_timings:        eval time =    7932.51 ms /    95 runs   (   83.50 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =   10276.94 ms /   146 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      11.75 ms /   127 runs   (    0.09 ms per token, 10812.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4028.67 ms /   199 tokens (   20.24 ms per token,    49.40 tokens per second)\n",
      "llama_print_timings:        eval time =   11368.55 ms /   126 runs   (   90.23 ms per token,    11.08 tokens per second)\n",
      "llama_print_timings:       total time =   15623.98 ms /   325 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      16.05 ms /   175 runs   (    0.09 ms per token, 10906.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2872.64 ms /   114 tokens (   25.20 ms per token,    39.68 tokens per second)\n",
      "llama_print_timings:        eval time =   14856.92 ms /   174 runs   (   85.38 ms per token,    11.71 tokens per second)\n",
      "llama_print_timings:       total time =   18053.28 ms /   288 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      17.31 ms /   180 runs   (    0.10 ms per token, 10401.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2161.75 ms /    48 tokens (   45.04 ms per token,    22.20 tokens per second)\n",
      "llama_print_timings:        eval time =   15111.00 ms /   179 runs   (   84.42 ms per token,    11.85 tokens per second)\n",
      "llama_print_timings:       total time =   17600.61 ms /   227 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      17.63 ms /   185 runs   (    0.10 ms per token, 10492.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2168.37 ms /    50 tokens (   43.37 ms per token,    23.06 tokens per second)\n",
      "llama_print_timings:        eval time =   15965.10 ms /   184 runs   (   86.77 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =   18481.74 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7677.86 ms\n",
      "llama_print_timings:      sample time =      23.27 ms /   256 runs   (    0.09 ms per token, 10999.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3787.24 ms /   177 tokens (   21.40 ms per token,    46.74 tokens per second)\n",
      "llama_print_timings:        eval time =   22520.34 ms /   255 runs   (   88.32 ms per token,    11.32 tokens per second)\n",
      "llama_print_timings:       total time =   26854.29 ms /   432 tokens\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Item\n__root__\n  Unterminated string starting at: line 23 column 1 (char 555) [type=value_error.jsondecode, input_value='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...tes Ltd\"\\n}\\n},\\n\"inter', input_type=str]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/pydantic/main.py:1143\u001b[0m, in \u001b[0;36mBaseModel.parse_raw\u001b[0;34m(cls, b, content_type, encoding, proto, allow_pickle)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1143\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_str_bytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/pydantic/deprecated/parse.py:49\u001b[0m, in \u001b[0;36mload_str_bytes\u001b[0;34m(b, content_type, encoding, proto, allow_pickle, json_loads)\u001b[0m\n\u001b[1;32m     48\u001b[0m         b \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mdecode(encoding)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m proto \u001b[38;5;241m==\u001b[39m Protocol\u001b[38;5;241m.\u001b[39mpickle:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 23 column 1 (char 555)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[340], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m expense_description \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(f\"Input: {expense_description}\\n\")\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m parsed_description \u001b[38;5;241m=\u001b[39m \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpense_description\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, separators\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     18\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(json_str)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Add a newline after each JSON object\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/llama_index/program/lmformatenforcer/base.py:103\u001b[0m, in \u001b[0;36mLMFormatEnforcerPydanticProgram.__call__\u001b[0;34m(self, llm_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcomplete(full_str, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_kwargs)\n\u001b[1;32m    102\u001b[0m text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/pydantic/main.py:1170\u001b[0m, in \u001b[0;36mBaseModel.parse_raw\u001b[0;34m(cls, b, content_type, encoding, proto, allow_pickle)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;66;03m# ctx is missing here, but since we've added `input` to the error, we're not pretending it's the same\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     error: pydantic_core\u001b[38;5;241m.\u001b[39mInitErrorDetails \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;66;03m# The type: ignore on the next line is to ignore the requirement of LiteralString\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: pydantic_core\u001b[38;5;241m.\u001b[39mPydanticCustomError(type_str, \u001b[38;5;28mstr\u001b[39m(exc)),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m'\u001b[39m,),\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: b,\n\u001b[1;32m   1169\u001b[0m     }\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m pydantic_core\u001b[38;5;241m.\u001b[39mValidationError\u001b[38;5;241m.\u001b[39mfrom_exception_data(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, [error])\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_validate(obj)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Item\n__root__\n  Unterminated string starting at: line 23 column 1 (char 555) [type=value_error.jsondecode, input_value='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...tes Ltd\"\\n}\\n},\\n\"inter', input_type=str]"
     ]
    }
   ],
   "source": [
    "with open('items.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Assuming the JSON structure is like {\"array_key\": [item1, item2, ...]}\n",
    "# Access the array within the JSON data\n",
    "array_items = data['rows']\n",
    "\n",
    "# Path to the JSON lines file\n",
    "file_path = \"output.jsonl\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, 'a') as file:\n",
    "    # Iterate over the items in the array\n",
    "    for index, item in enumerate(array_items):\n",
    "        expense_description = item[1]\n",
    "        # print(f\"Input: {expense_description}\\n\")\n",
    "        parsed_description = program(item=expense_description).json(indent=None, separators=(',', ':'))\n",
    "        file.write(f\"{parsed_description}\\n\")\n",
    "        # print(json_str)\n",
    "        # Add a newline after each JSON object\n",
    "        file.flush()\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(donation=Donation(donor=None, amount=Money(value=100.0, currency='CAD'), date=None, description='A donation of $100 to support the local animal shelter'), interest=None, registered_date=None)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json.dumps((json_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
